{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASIFICADOR DE IMAGENES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "primero hacerlo y probarlo como lo hacen en algun ejemplo de internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "despues recolectar datos (usando lo que paso willy + Google formularies) para hacer el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usar esa red pero con los datos recolectados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer crossvalidation o algo asi para ajustar las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Funciones y modelos que vamos a usar de KEras \n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matar las sesiones anteriores de Keras\n",
    "from tensorflow.python.keras import backend as K \n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino ruta a los datos que voy a usar\n",
    "data_entrenamiento = './data/entrenamiento'\n",
    "data_validacion = './data/validacion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros de la red y de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epocas, son como las vueltas que da\n",
    "epocas=5 #20\n",
    "\n",
    "#tamaño de imagenes, 150x150 pixels\n",
    "longitud, altura = 150, 150 \n",
    "\n",
    "#cantidad de imagenes que va procesar en cada epoca (tamaño de lotes)\n",
    "batch_size = 32\n",
    "\n",
    "#numero de veces que se va a procesar la informacion en cada epoca\n",
    "pasos = 100 #1000\n",
    "\n",
    "#numero de veces que se va a validar la informacion en cada epoca\n",
    "validation_steps = 300\n",
    "\n",
    "#Profundidad de las dos capas de convolucion\n",
    "filtrosConv1 = 32\n",
    "filtrosConv2 = 64\n",
    "\n",
    "#tamaño de los filtros que vamos a usar en las convoluciones\n",
    "tamano_filtro1 = (3, 3)\n",
    "tamano_filtro2 = (2, 2)\n",
    "tamano_pool = (2, 2)\n",
    "\n",
    "#Tipo de clases, en este ejemplo --> gatos, perros y gorillas\n",
    "clases = 3\n",
    "\n",
    "#\"Learning Rate\" --> tamaño de ajuste a la red para acercarse a la mejor solucion\n",
    "lr = 0.0004\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos nuestras imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 images belonging to 3 classes.\n",
      "Found 2043 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#defino generadores de datos\n",
    "\n",
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255, #rescalar las imagenes --> en vez de 0 a 255, lo cambio para que los valores esten entr 0 y 1\n",
    "    shear_range=0.2, # va a inclinar las imagenes aleatoriamente\n",
    "    zoom_range=0.2, # va a ser zoom aleatorios\n",
    "    horizontal_flip=True) #invierte la imagen aleatoriamente\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255) # para la validacion no queremos que modifique la imagen (solo el rescalado)\n",
    "\n",
    "#Extraigo imagenes\n",
    "\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento, # la ruta a los datos de entrenamiento\n",
    "    target_size=(altura, longitud), # lo que definimos antes\n",
    "    batch_size=batch_size, # tamaño de lotes que ya definimos\n",
    "    class_mode='categorical') #clasificacion categorica --> es decir que es por clases\n",
    "\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiendo parametros estructurales de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red secuencial, varias capas apiladas\n",
    "cnn = Sequential()\n",
    "\n",
    "#Primera Capa --> convolucion\n",
    "cnn.add(Convolution2D(   #defino la primera capa --> de convolucion\n",
    "    filtrosConv1, # 32 filtros\n",
    "    tamano_filtro1, \n",
    "    padding =\"same\", \n",
    "    input_shape=(longitud, altura, 3), # lo que va a entrar (obs, el 3 es porque es rgb)\n",
    "    activation='relu')) #relu es la funcion de activacion de la neurona\n",
    "\n",
    "#Segunda capa --> Max pooling\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))  #agrupa pixeles y agarra el valor del pixel mas grande\n",
    "\n",
    "#Tercera capa --> convolucion\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\"))\n",
    "\n",
    "\n",
    "#Cuarta capa --> Max pooling\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool)) #agrupa pixeles (en este caso era de 2x2) y agarra el valor del pixel mas grande\n",
    "\n",
    "\n",
    "cnn.add(Flatten()) # aplana la imagen, 1 sola dimension\n",
    "\n",
    "\n",
    "#Quinta capa-->  Dense\n",
    "cnn.add(Dense(\n",
    "    256, #256  neuronas \"normales\"\n",
    "    activation='relu')) #con activacion relu\n",
    "\n",
    "\n",
    "cnn.add(Dropout(0.5)) # durate el entrenamiento le apago el 0,5 de las neuronas --> esto es para evitar el overfitting\n",
    "\n",
    "\n",
    "#Sexta capa -->Dense --> ULTIMA CAPA --> tiene 3 neuronas porque son 3 clases\n",
    "cnn.add(Dense(clases, activation='softmax')) # la funcion softmax te va a decir que probabilidad de que sea cada clase\n",
    "                                            # Softmax es utilizada en la capa final de los clasificadores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Este es el que me va a ir ajustando el error para ir entrenando la red    \n",
    "cnn.compile(loss='categorical_crossentropy', #te dice que tan bien le esta yendo\n",
    "            optimizer=optimizers.Adam(lr=lr), #optimizador Adam --> ajusta los parametros\n",
    "            metrics=['accuracy']) #lo que trata de mejorar, en este caso el porcentaje de imagenes que acerto --> accuracy\n",
    "\n",
    "# El proceso de entrenamiento, con epocas, lotes, etc\n",
    "cnn.fit_generator(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=pasos,\n",
    "    epochs=epocas,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar Modelo Entrenado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-608032a679b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./modelo/modelo.h5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# guarda la estructura del modelo en si\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./modelo/pesos.h5'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# guarda los pesos de las variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "target_dir = './modelo/' # geero carpeta \"modelo\"\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "\n",
    "cnn.save('./modelo/modelo.h5') # guarda la estructura del modelo en si\n",
    "cnn.save_weights('./modelo/pesos.h5') # guarda los pesos de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d64a7c990fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodelo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./modelo/modelo.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpesos_modelo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./modelo/pesos.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpesos_modelo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    184\u001b[0m   \"\"\"\n\u001b[0;32m    185\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_model` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: `load_model` requires h5py."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "longitud, altura = 150, 150\n",
    "modelo = './modelo/modelo.h5'\n",
    "pesos_modelo = './modelo/pesos.h5'\n",
    "cnn = load_model(modelo)\n",
    "cnn.load_weights(pesos_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file):\n",
    "  x = load_img(file, target_size=(longitud, altura))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  array = cnn.predict(x)\n",
    "  result = array[0]\n",
    "  answer = np.argmax(result)\n",
    "  if answer == 0:\n",
    "    print(\"pred: Perro\")\n",
    "  elif answer == 1:\n",
    "    print(\"pred: Gato\")\n",
    "  elif answer == 2:\n",
    "    print(\"pred: Gorila\")\n",
    "\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-30114504f04e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/abc.jpg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "abc= cv2.imread('./data/abc.jpg',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Gorila\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('./data/entrenamiento/gorila/20.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
